<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Runkai Zheng | Home</title>
  <meta name="description" content="Homepage of Runkai Zheng">
  <meta name="author" content="Runkai Zheng">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=../libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=../libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=../libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academic-Icons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/academicons-1.8.6/css/academicons.min.css>


  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=../libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=../libs/external/github-prettify-theme.css>-->
  <script src=../libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=../libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=../libs/icon.png>

  <!-- Music player -->
  <link rel="stylesheet" href="../libs/custom/audioplayer.css">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous">
    </script>
  <script src="../libs/custom/audioplayer.js"></script>
  <!-- Google Analytics -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7YM94T4RDX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-7YM94T4RDX');
  </script>



</head>

<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <!-- <a href="/"><img title="my lovely cat" class="u-max-full-width" src='../assets/profile-pics/huida.jpg'></a> -->
          <a href="/"><img title="Me" class="u-max-full-width" src='../assets/profile-pics/me.jpg'></a>
        </div>
        <div class="nine columns main-description">
          <h1>Runkai Zheng</h1>
          <p>(he/him/his)</p>
          <p>Master student, The Chinese University of Hong Kong, Shenzhen</p>
          <p>Building 2, No. 5 Dan Ling Street, Haidian District, Beijing</p>
          <p>rkteddy [AT] outlook.com</p>
          <p>
            <span onclick="window.open('https://github.com/rkteddy')" style="cursor: pointer">
              <i class="fa fa-github" aria-hidden="true"></i>
            </span>

            <span onclick="window.open('https://scholar.google.com/citations?user=52haRQ0AAAAJ&hl=en')"
              style="cursor: pointer">
              <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
            </span>

            <span onclick="window.open('https://orcid.org/0000−0003−3120−5466')" style="cursor: pointer">
              <i class="ai ai-orcid ai-lg" aria-hidden="true"></i>
            </span>
          </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=../index.html#about>About</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#research>Research</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#publications>Publications</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#hardwareprojects>Hardware Projects</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#misc>Misc</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=index.html#meta>Meta</a></li> -->
        </ul>
      </div>
    </nav>

    <script>
      function unhide(divID) {
        console.log(divID)
        var item = document.getElementById(divID);
        console.log('hi')
        if (item) {
          item.className = (item.className == 'hidden') ? 'unhidden' : 'hidden';
        }
      }
    </script>

    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
      type="text/javascript"></script>

    <!-- ========== ABOUT ========== -->

    <div class="docs-section">

      <div class="title-subtitle">
        <h3>Categorial Informational Bottleneck</h3>
        <h5>A Game-theoretic solver-based regularization on vision classifier.</h5>
      </div>

      <h5 id="motivation"><b>Motivation</b></h5>

      <p>How to learn a good representation for visual inputs has been a controversial problem. A relevant example to this work is Variation Informational Bottleneck (VIB). VIB typically pose an assumption that a good representation should contain as less information about the input as possible, in the presence of correct classification. This could be intuitively understood because we won't consider a too complex representation as a classification basis, which could potentially lead to overfitting. According to Occam's razor, simple yet
        sufficient representation should be generalized better. In this work, we present another interesting assumption, that is, a good representation should contain as less information about the non-target class as possible, and of course, under the premise of correct classification.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/illustration.png" style="width: 50%" />
        <figcaption><b>.</b></figcaption>
      </figure>

      <p>The intuition is illustrated above. We select three images of three different categories from Caltech CUB-200. Let's look at the Red Winged Blackbird (Class A), whose unique feature is the red & white marking. Unique feature means that such feature doesn't exists in the other two categories: Blewer Blackbird and Rusty Blackbird (Class B and C). Hence, we should expect such a feature be considered as one of the most important basis for classification. We select the channel (single dimension in the representation vector, which is obtained by pooling the feature map in the penultimate layer) with the highest activation value w.r.t Red Winged Blackbird, and upscale then map the corresponding feature map to the input image. We can see that the attention does focus on the red&white marking. This is fine. However, when we change the input images into bird of the other two categories, we find the same channel also activated. This is not as we expected. Because if the channel is responsible to detect the red&white marking, it should not be activated when the input image has not such pattern. That means the channel contains not only the information about the red & white marking. Therefore, our work is to squeeze out those irrelevant information, and specialize the channels such that they are all responsible to detect one of unique patterns about the target class. Such specialized channels will remain silence when the target class is not presence.</p>

      <h5 id="formulation"><b>Formulation</b></h5>

      <p>The problem formulation can be written as minimizing the mutual information (MI) between predicted output of non-target classes $\hat{Y}_{-t}$ and extracted features $\Phi(X^t)$, i.e.,
        \begin{equation} I_{\theta}(\hat{Y}_{-t}; \Phi(X^t))), \end{equation}
      where $X^t$ is the input from class $t$ and $\hat{Y}_{-t}$ is the predictive probability of $X^t$ excluding the target class $t$.</p>
      <p>With steps of derivation (please refer to the <a href="https://arxiv.org/abs/2011.10951">paper</a> for more details), we can finally reach an upper bound of the objective, i.e.,</p>
      \begin{equation} I_{\theta}(\hat{Y}_{-t}; \Phi(X^t))) \leq C+H_{\theta}(\hat{Y}_{-t}|X^t)$, \end{equation}
      where $C$ is a constant and $H$ denotes entropy. In order to achieve the goal, one should maximize the empirical conditional entropy of the predictive distribution over the non-target categories.</p>

      <h5 id="method"><b>Method</b></h5>
      <p>However, we find the objective cannot be sufficiently optimized since the gradient vanishes when the conditional entropy nearly reach it maximal. To solve this problem, we propose a game theoretic framework, whose Nash equilibrium is proved to be a feasible solution of our goal. And most importantly, the game theoretic objective can be optimized highly efficiently.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/framework.png" style="width: 100%" />
        <figcaption><b>.</b></figcaption>
      </figure>

      <p>Specifically, we define a zero-sum strategic game played between the model and a designed adversary with loss of the model defined as the cross entropy loss $D_{CE}(p||q)$. We let $p$ be the ground truth label vector, normally one-hot encoding, while in our work we specifically design it for the objective. Here, $p$ is the strategy of the adversary and q is the strategy of the model. We assume that the model is a classifier with confidence $q_t$ on the target class. The model aims to assign the rest of the probability $1 − q_t$ to non-target classes to minimize the loss. The adversary is the controller of the ground truth with fixed $p_t$ for the target class, and it aims to maximize the loss via adjusting the distribution on non-target class of $p$. This is a dynamic game that the two players play in order, in which the model goes first, and the adversary can adjust the strategy according to the previous action of the model.</p>

      <p>We proved three theorems respectively giving the worst-case payoff $D_{CE}(p^*||q)$, the best response $q^*$, and the Nash equilirium of the game. Since the Nash equilibrium in a two player zero-sum game is equivalent to a minimax solution. Thus, by training with the worst-case payoff $D_{CE}(p^*||q)$, we expect that the model output ultimately converges to the best response $q^*$. Finally, our proposed <b>minimax loss (MM)</b> is defined as:
      \begin{align}
      \mathcal{L}_{MM} (p_t)
      &= \mathbb{E}_{x\sim\mathcal{X}}[ D_{CE}(p^*||q)]
      = \mathbb{E}_{x\sim\mathcal{X}}[-p_t\log q(x;\theta)_t -(1-p_t)\log q(x;\theta)_k].
      \end{align} </p>
      <h5 id="visualization"><b>Visualization</b></h5>

      <p>We evaluate the proposed model in the abovementioned scenario on CUB-200. This time, we select the two channels with top activation values on class A. The two channels in a standard cross entropy (CE) trained model fails to activate uniquely according to the red & white marking. In MM trained model, the two top channels are only activated for the red & white marking, and remain silence (activation values are low) when the input image doesn't contain that target pattern (B and C). So far we have reached our goal and obtained a class unique feature extractor.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/performance_0.png" style="width: 75%" />
        <figcaption><b></b></figcaption>
      </figure>


      <!-- <h5 id="discussion"><b>Discussion</b></h5>

      <p>Variation Informational Bottelneck (VIB) use a variational upper bound on the , to squeeze the information in
        the representations about the input images in the presense of correct classification. Specifically, VIB takes
        the below formulation as the objective:</p>
      <p>Different from VIB, our categorical informational bottleneck instead squeeze out the information about
        irrelevant categories:</p> -->


      <p><strong>Relevant Publications:</strong></p>
      <ol>
        <li>"Learning Class Unique Feature in Fine-grained Visual Classification”, arXiv, preprint.</li>
      </ol>
    </div>

    <div class="footer">
      <div class="row">
        <div class="four columns">
          Runkai Zheng
        </div>
        <div class="four columns">
          rkteddy [AT] outlook.com
        </div>
        <div class="four columns">
          <span onclick="window.open('https://github.com/rkteddy')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>

          <span onclick="window.open('https://scholar.google.com/citations?user=52haRQ0AAAAJ&hl=en')"
            style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>

          <span onclick="window.open('https://orcid.org/0000−0003−3120−5466')" style="cursor: pointer">
            <i class="ai ai-orcid ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

    <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>