<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Runkai Zheng | Home</title>
  <meta name="description" content="Homepage of Runkai Zheng">
  <meta name="author" content="Runkai Zheng">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=../libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=../libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=../libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academic-Icons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/academicons-1.8.6/css/academicons.min.css>


  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=../libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=../libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=../libs/external/github-prettify-theme.css>-->
  <script src=../libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=../libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=../libs/icon.png>

  <!-- Music player -->
  <link rel="stylesheet" href="../libs/custom/audioplayer.css">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous">
    </script>
  <script src="../libs/custom/audioplayer.js"></script>
  <!-- Google Analytics -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7YM94T4RDX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-7YM94T4RDX');
  </script>



</head>

<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <!-- <a href="/"><img title="my lovely cat" class="u-max-full-width" src='../assets/profile-pics/huida.jpg'></a> -->
          <a href="/"><img title="Me" class="u-max-full-width" src='../assets/profile-pics/me.jpg'></a>
        </div>
        <div class="nine columns main-description">
          <h1>Runkai Zheng</h1>
          <p>(he/him/his)</p>
          <p>Master student, The Chinese University of Hong Kong, Shenzhen</p>
          <p>Building 2, No. 5 Dan Ling Street, Haidian District, Beijing</p>
          <p>rkteddy [AT] outlook.com</p>
          <p>
            <span onclick="window.open('https://github.com/rkteddy')" style="cursor: pointer">
              <i class="fa fa-github" aria-hidden="true"></i>
            </span>

            <span onclick="window.open('https://scholar.google.com/citations?user=52haRQ0AAAAJ&hl=en')"
              style="cursor: pointer">
              <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
            </span>

            <span onclick="window.open('https://orcid.org/0000−0003−3120−5466')" style="cursor: pointer">
              <i class="ai ai-orcid ai-lg" aria-hidden="true"></i>
            </span>
          </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=../index.html#about>About</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#research>Research</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#publications>Publications</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#hardwareprojects>Hardware Projects</a></li>
          <li class="navbar-item"><a class="navbar-link" href=../index.html#misc>Misc</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=index.html#meta>Meta</a></li> -->
        </ul>
      </div>
    </nav>

    <script>
      function unhide(divID) {
        console.log(divID)
        var item = document.getElementById(divID);
        console.log('hi')
        if (item) {
          item.className = (item.className == 'hidden') ? 'unhidden' : 'hidden';
        }
      }
    </script>

    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
      type="text/javascript"></script>

    <!-- ========== ABOUT ========== -->

    <div class="docs-section">

      <div class="title-subtitle">
        <h3>Categorial Informational Bottleneck</h3>
        <h5>A Game-theoretic solver-based regularization on vision classifier.</h5>
      </div>

      <h5 id="motivation"><b>Motivation</b></h5>

      <p>The problem of learning good representations for visual inputs has been a topic of controversy. A relevant solution to this is Variation Informational Bottleneck (VIB), which typically assumes that a good representation should contain as little information about the input as possible while still allowing for correct classification. This can be intuitively understood because a complex representation may lead to overfitting and poor generalization, according to Occam's razor. In this work, we propose a different assumption: a good representation should contain as little information about the non-target class as possible, while still allowing for correct classification.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/illustration.png" style="width: 50%" />
        <figcaption><b>Figure 1. Three images of similar bird species from CUB-200-2011 dataset are selected. The first row presents the original images and the second row presents the activation maps. The activation maps are obtained from forward propagating the image of Red Winged Blackbird to the CE trained CNN and extracting the penultimate layer feature maps. Then the channel that has the largest mean activation value is selected. An observation is that the same channel also has high activation value when we input the other two categories of images (Brewer Blackbird and Rusty Blackbird)</b></figcaption>
      </figure>

      <p>The intuition behind this approach is illustrated in the example above. We selected three images of three different categories from Caltech CUB-200-2011. Let's consider the Red Winged Blackbird (Class A), whose unique feature is the red and white marking. By unique feature, we mean that such a feature does not exist in the other two categories: Blewer Blackbird and Rusty Blackbird (Class B and C). Therefore, we should expect this feature to be considered as one of the most important bases for classification. We selected the channel (a single dimension in the representation vector, which is obtained by pooling the feature map in the penultimate layer) with the highest activation value with respect to the Red Winged Blackbird, and upscaled the corresponding feature map to the input image. We can see that the attention does focus on the red and white markings. This is expected. However, when we change the input images to birds from the other two categories, we find that the same channel is also activated. This is not what we expected, because if the channel is responsible for detecting the red and white markings, it should not be activated when the input image does not have such a pattern. This means that the channel contains information not only about the red and white markings, but also about other aspects of the image. Our goal is to eliminate this irrelevant information and specialize the channels such that each is only responsible for detecting one unique pattern of the target class. This way, the specialized channels will remain silent when the target class is not present in the image.</p>

      <h5 id="formulation"><b>Formulation</b></h5>

      <p>The problem formulation can be written as minimizing the mutual information (MI) between predicted output of non-target classes $\hat{Y}_{-t}$ and extracted features $\Phi(X^t)$, i.e.,
        \begin{equation} I_{\theta}(\hat{Y}_{-t}; \Phi(X^t))), \end{equation}
      where $X^t$ is the input from class $t$ and $\hat{Y}_{-t}$ is the predictive probability of $X^t$ excluding the target class $t$.</p>
      <p>With steps of derivation (please refer to the <a href="https://arxiv.org/abs/2011.10951">paper</a> for more details), we can finally reach an upper bound of the objective, i.e.,</p>
      \begin{equation} I_{\theta}(\hat{Y}_{-t}; \Phi(X^t))) \leq C+H_{\theta}(\hat{Y}_{-t}|X^t), \end{equation}
      where $C$ is a constant and $H$ denotes entropy. In order to achieve the goal, one should maximize the empirical conditional entropy of the predictive distribution over the non-target categories.</p>

      <h5 id="method"><b>Method</b></h5>
      <p>However, we find the objective cannot be sufficiently optimized since the gradient vanishes when the conditional entropy nearly reach it maximal. To solve this problem, we propose a game theoretic framework, whose Nash equilibrium is proved to be a feasible solution of our goal. And most importantly, the game theoretic objective can be optimized highly efficiently.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/framework.png" style="width: 100%" />
        <figcaption><b>Figure 2. (a) During the training phase (before convergence), the model keep promoting the smallest probability in the non-target model output by assign $1-p_{t}$ to the index of the minimum value in $q_{C\setminus t}$, i.e.., $q_{3}$ in (a). (b) After iterations of training, the model output distribution will finally be uniformly distributed over non-target classes. When reaching a convergence, a Nash equilibrium exists between the optimal solution of the model and the adversary.</b></figcaption>
      </figure>

      <p>Specifically, we define a zero-sum strategic game played between a model and a designed adversary, with the loss of the model defined as the cross entropy loss $D_{CE}(p||q)$. In this game, $p$ represents the strategy of the adversary and $q$ represents the strategy of the model. We assume that the model is a classifier with confidence $q_t$ on the target class. The model aims to assign the rest of the probability $1-q_t$ to non-target classes in order to minimize the loss. The adversary, on the other hand, controls the ground truth with a fixed $p_t$ for the target class and aims to maximize the loss by adjusting the distribution on the non-target classes of $p$. This is a dynamic game that the two players take turns playing, with the model going first and the adversary adjusting its strategy according to the previous action of the model.</p>

      <p>We proved three theorems respectively giving the worst-case payoff $D_{CE}(p^*||q)$, the best response $q^*$, and the Nash equilirium of the game. Since the Nash equilibrium in a two player zero-sum game is equivalent to a minimax solution. Thus, by training with the worst-case payoff $D_{CE}(p^*||q)$, we expect that the model output ultimately converges to the best response $q^*$. Finally, our proposed <b>minimax loss (MM)</b> is defined as:
      \begin{align}
      \mathcal{L}_{MM} (p_t)
      &= \mathbb{E}_{x\sim\mathcal{X}}[ D_{CE}(p^*||q)]
      = \mathbb{E}_{x\sim\mathcal{X}}[-p_t\log q(x;\theta)_t -(1-p_t)\log q(x;\theta)_k].
      \end{align} </p>
      <h5 id="visualization"><b>Visualization</b></h5>

      <p>We evaluate the proposed model in the scenario described above on CUB-200-2011 in Figure 3. This time, we selected the two channels with the highest activation values for Class A. In a standard cross entropy (CE) trained model, the two channels fail to activate uniquely according to the red and white markings. However, in the MM trained model, the two top channels are only activated for the red and white markings, and remain silent (low activation values) when the input image does not contain that target pattern (Class B and C). We have thus achieved our goal of obtaining a class-unique feature extractor.</p>

      <figure class="image">
        <img src="../assets/projects/cuf/performance_0.png" style="width: 60%" />
        <figcaption><b>Figure 3. Visualization on CUB-200-2011.</b></figcaption>
      </figure>

      We demonstrate a visualization on the CIFAR-10 dataset in Figure 4 to give a better sense of the class-unique features. The top row shows the original images and their corresponding categories. The bottom row shows the activations on the penultimate layer's 128 feature maps for the standard CE trained and MM trained ResNet-18 models, where each map has 8 x 8 grids and each collection contains 8 x 16 maps. It is clear that the MM trained CNN generates sparse class-unique feature maps: the three different classes each have their own distinct subsets of feature maps that will only be activated for the corresponding classes, and the three different airplanes activate almost the same subset of feature maps.
      <figure class="image">
        <img src="../assets/projects/cuf/performance_1.png" style="width: 75%" />
        <figcaption><b>Figure 4. Visualization on CIFAR-10.</b></figcaption>
      </figure>


      <!-- <h5 id="discussion"><b>Discussion</b></h5>

      <p>Variation Informational Bottelneck (VIB) use a variational upper bound on the , to squeeze the information in
        the representations about the input images in the presense of correct classification. Specifically, VIB takes
        the below formulation as the objective:</p>
      <p>Different from VIB, our categorical informational bottleneck instead squeeze out the information about
        irrelevant categories:</p> -->


      <p><strong>Relevant Publications:</strong></p>
      <ol>
        <li>"Learning Class Unique Feature in Fine-grained Visual Classification”, arXiv, preprint.</li>
      </ol>
    </div>

    <div class="footer">
      <div class="row">
        <div class="four columns">
          Runkai Zheng
        </div>
        <div class="four columns">
          rkteddy [AT] outlook.com
        </div>
        <div class="four columns">
          <span onclick="window.open('https://github.com/rkteddy')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>

          <span onclick="window.open('https://scholar.google.com/citations?user=52haRQ0AAAAJ&hl=en')"
            style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>

          <span onclick="window.open('https://orcid.org/0000−0003−3120−5466')" style="cursor: pointer">
            <i class="ai ai-orcid ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

    <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>